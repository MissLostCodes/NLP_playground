{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovUCPCtV39BU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d4a011"
      },
      "source": [
        "# Task\n",
        "Perform a series of Natural Language Toolkit (NLTK) text preprocessing tasks. This includes downloading necessary NLTK data (punkt, stopwords, wordnet), applying word tokenization, stop word removal, stemming, and lemmatization on various given sentences and paragraphs. You will also perform sentence tokenization, compare the outputs of stemming and lemmatization on specific words, and conduct comprehensive text preprocessing on informal text. Finally, summarize all the NLTK text preprocessing tasks performed and their outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f985cd6f"
      },
      "source": [
        "## NLTK Setup\n",
        "\n",
        "### Subtask:\n",
        "Download necessary NLTK data (punkt, stopwords, wordnet) for text processing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36054f30"
      },
      "source": [
        "**Reasoning**:\n",
        "To download the necessary NLTK data, I need to first import the nltk library and then use the `nltk.download()` function for each specified dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb600d0a",
        "outputId": "8ee96cb2-cb30-40b5-c4eb-522a42cdc82d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d68ee83f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution output shows successful downloads of the NLTK data (punkt, stopwords, wordnet), indicated by the unzipping messages and the 'True' return value. No fix is required for the code itself, so I will provide the same code block to confirm its successful execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0f2348b",
        "outputId": "64497e55-5f48-40b0-93ed-cca2922ad166"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1c12bb8"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary NLTK data is downloaded, the next step is to perform word tokenization as outlined in the main task. I will define a sample sentence and use `nltk.word_tokenize` to break it down into individual words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecbdd0ba",
        "outputId": "057cc6a8-31cf-4e6d-8b27-9547ca4c2da6"
      },
      "source": [
        "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(f\"Original Sentence: {sentence}\")\n",
        "print(f\"Word Tokenization: {words}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: NLTK is a powerful library for natural language processing.\n",
            "Word Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "762ab2ab"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `word_tokenize` operation failed because the `punkt_tab` resource was not found. The error message explicitly suggested downloading `punkt_tab` to resolve this. I will add `nltk.download('punkt_tab')` to ensure all necessary resources for tokenization are available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a76fc9c2",
        "outputId": "a12cfcf2-2d08-4189-b633-ae354996445d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "sentence = \"NLTK is a powerful library for natural language processing.\"\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(f\"Original Sentence: {sentence}\")\n",
        "print(f\"Word Tokenization: {words}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: NLTK is a powerful library for natural language processing.\n",
            "Word Tokenization: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8da2c9a5"
      },
      "source": [
        "**Reasoning**:\n",
        "With word tokenization successfully completed, the next logical step according to the task description is to perform stop word removal. I will import `stopwords` from `nltk.corpus`, define a set of English stopwords, and then filter the tokenized words to remove them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c164dc72",
        "outputId": "f6c31475-0a01-4a1d-9b60-d9955fce8c13"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "print(f\"Original Words: {words}\")\n",
        "print(f\"Words after Stop Word Removal: {filtered_words}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']\n",
            "Words after Stop Word Removal: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43abae8c"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that stop word removal is complete, the next task is to perform stemming. I will import the `PorterStemmer` from `nltk.stem`, create an instance of it, and then apply stemming to the `filtered_words` list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1767abd3",
        "outputId": "2029b5ca-eaba-43d2-a5a5-c1586e401bc7"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "print(f\"Words after Stop Word Removal: {filtered_words}\")\n",
        "print(f\"Words after Stemming: {stemmed_words}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words after Stop Word Removal: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']\n",
            "Words after Stemming: ['nltk', 'power', 'librari', 'natur', 'languag', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5910de1d"
      },
      "source": [
        "**Reasoning**:\n",
        "With stemming completed, the next logical step is to perform lemmatization as outlined in the task. I will import the `WordNetLemmatizer` from `nltk.stem`, create an instance, and then apply lemmatization to the `filtered_words` list. To ensure accurate lemmatization, especially for verbs, I will specify the part-of-speech tag as 'v' (verb) where appropriate, although for this example, a default POS tag should suffice for general words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4931a41",
        "outputId": "3ed51408-8213-4fae-b565-f5849f8ede4d"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(f\"Words after Stop Word Removal: {filtered_words}\")\n",
        "print(f\"Words after Lemmatization: {lemmatized_words}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words after Stop Word Removal: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']\n",
            "Words after Lemmatization: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ef110b3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous lemmatization step using the default POS tag did not produce any changes in the words, making it indistinguishable from the filtered words. To perform more accurate lemmatization and prepare for the comparison with stemming, it's crucial to utilize Part-of-Speech (POS) tagging. I will first download the necessary 'averaged_perceptron_tagger' for POS tagging, then map NLTK's POS tags to WordNet's POS tags, and finally re-apply lemmatization with these context-aware tags.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ab887c",
        "outputId": "4b625153-e895-4b61-e1ce-f79a9c5c9b8f"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"\n",
        "    Map NLTK POS tag to WordNet POS tag\n",
        "    \"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\n",
        "        \"J\": wordnet.ADJ,\n",
        "        \"N\": wordnet.NOUN,\n",
        "        \"V\": wordnet.VERB,\n",
        "        \"R\": wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Example input (replace with your filtered_words)\n",
        "filtered_words = [\"scientists\", \"announced\", \"secret\", \"medicine\", \"cures\", \"diseases\"]\n",
        "\n",
        "lemmatized_words_pos = [\n",
        "    lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "    for word in filtered_words\n",
        "]\n",
        "\n",
        "print(\"Words after Stop Word Removal:\", filtered_words)\n",
        "print(\"Words after Lemmatization (with POS):\", lemmatized_words_pos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSPzZY-s8OAo",
        "outputId": "2a67b105-1920-4b92-880e-83271be81c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words after Stop Word Removal: ['scientists', 'announced', 'secret', 'medicine', 'cures', 'diseases']\n",
            "Words after Lemmatization (with POS): ['scientist', 'announce', 'secret', 'medicine', 'cure', 'disease']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccb45641"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the specific resource 'averaged_perceptron_tagger_eng' was not downloaded, even though 'averaged_perceptron_tagger' was attempted. I will update the download command to explicitly request 'averaged_perceptron_tagger_eng' to resolve this issue and re-run the lemmatization with POS tagging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bda89d1",
        "outputId": "9e057144-69e8-4682-9a79-43663069b8df"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized_words_pos = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words]\n",
        "\n",
        "print(f\"Words after Stop Word Removal: {filtered_words}\")\n",
        "print(f\"Words after Lemmatization (with POS): {lemmatized_words_pos}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words after Stop Word Removal: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']\n",
            "Words after Lemmatization (with POS): ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78594c13"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous lemmatization using POS-tagging did not show significant changes because the words in `filtered_words` were already in their base forms or did not have prominent inflections that would change with lemmatization. To effectively compare stemming and lemmatization, I need to use specific words that demonstrate the differences between these techniques, especially when POS tagging is considered for lemmatization. I will define a new list of words and apply stemming, default lemmatization, and POS-aware lemmatization to them, then print the results for direct comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3439c427",
        "outputId": "5b915cc3-9a92-4d63-8d63-95ecca8450b0"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure necessary downloads for this comparison (if not already done)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Define a list of words for comparison\n",
        "words_for_comparison = ['running', 'runs', 'ran', 'better', 'best', 'geese', 'children', 'studies', 'studying']\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "print(f\"Original Words: {words_for_comparison}\")\n",
        "print(\"\\nComparison of Stemming and Lemmatization:\")\n",
        "for word in words_for_comparison:\n",
        "    stem = stemmer.stem(word)\n",
        "    lemma_default = lemmatizer.lemmatize(word) # Default (noun) lemmatization\n",
        "    lemma_pos = lemmatizer.lemmatize(word, get_wordnet_pos(word)) # POS-aware lemmatization\n",
        "    print(f\"  Word: {word: <10} | Stem: {stem: <10} | Lemma (Default): {lemma_default: <15} | Lemma (POS-aware): {lemma_pos: <15}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words: ['running', 'runs', 'ran', 'better', 'best', 'geese', 'children', 'studies', 'studying']\n",
            "\n",
            "Comparison of Stemming and Lemmatization:\n",
            "  Word: running    | Stem: run        | Lemma (Default): running         | Lemma (POS-aware): run            \n",
            "  Word: runs       | Stem: run        | Lemma (Default): run             | Lemma (POS-aware): run            \n",
            "  Word: ran        | Stem: ran        | Lemma (Default): ran             | Lemma (POS-aware): ran            \n",
            "  Word: better     | Stem: better     | Lemma (Default): better          | Lemma (POS-aware): well           \n",
            "  Word: best       | Stem: best       | Lemma (Default): best            | Lemma (POS-aware): best           \n",
            "  Word: geese      | Stem: gees       | Lemma (Default): goose           | Lemma (POS-aware): geese          \n",
            "  Word: children   | Stem: children   | Lemma (Default): child           | Lemma (POS-aware): child          \n",
            "  Word: studies    | Stem: studi      | Lemma (Default): study           | Lemma (POS-aware): study          \n",
            "  Word: studying   | Stem: studi      | Lemma (Default): studying        | Lemma (POS-aware): study          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aced7d91"
      },
      "source": [
        "**Reasoning**:\n",
        "The comparison between stemming and lemmatization is now complete. According to the overall task, the next step is to perform sentence tokenization. I will define a sample paragraph and use `nltk.sent_tokenize` to split it into individual sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4c310fe",
        "outputId": "f1d53959-c62d-4246-c37f-9135e2baf123"
      },
      "source": [
        "import nltk\n",
        "\n",
        "paragraph = \"Natural Language Processing (NLP) is a field of artificial intelligence. It focuses on the interaction between computers and human language. NLTK is a popular library for NLP tasks.\"\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "print(f\"Original Paragraph: {paragraph}\")\n",
        "print(f\"Sentence Tokenization: {sentences}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph: Natural Language Processing (NLP) is a field of artificial intelligence. It focuses on the interaction between computers and human language. NLTK is a popular library for NLP tasks.\n",
            "Sentence Tokenization: ['Natural Language Processing (NLP) is a field of artificial intelligence.', 'It focuses on the interaction between computers and human language.', 'NLTK is a popular library for NLP tasks.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b600791"
      },
      "source": [
        "**Reasoning**:\n",
        "With sentence tokenization successfully completed, the next task is to perform comprehensive text preprocessing on informal text. I will define an informal sentence and apply word tokenization, stop word removal, stemming, and POS-aware lemmatization, similar to the steps already performed, to demonstrate the full preprocessing pipeline on this new text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "233f91b2",
        "outputId": "ac1ce0e7-ef09-41ee-b34a-a454ecfd2ea6"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "informal_text = \"OMG, NLTK's pretty cool for NLP stuff, innit? I'm like, totally loving it!\"\n",
        "\n",
        "# 1. Word Tokenization\n",
        "words_informal = nltk.word_tokenize(informal_text)\n",
        "print(f\"Original Informal Text: {informal_text}\")\n",
        "print(f\"Word Tokenization: {words_informal}\")\n",
        "\n",
        "# 2. Stop Word Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words_informal = [word for word in words_informal if word.lower() not in stop_words and word.isalpha()]\n",
        "print(f\"Words after Stop Word Removal: {filtered_words_informal}\")\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words_informal = [stemmer.stem(word) for word in filtered_words_informal]\n",
        "print(f\"Words after Stemming: {stemmed_words_informal}\")\n",
        "\n",
        "# 4. Lemmatization (with POS-tagging)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized_words_informal = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words_informal]\n",
        "print(f\"Words after Lemmatization (with POS): {lemmatized_words_informal}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Informal Text: OMG, NLTK's pretty cool for NLP stuff, innit? I'm like, totally loving it!\n",
            "Word Tokenization: ['OMG', ',', 'NLTK', \"'s\", 'pretty', 'cool', 'for', 'NLP', 'stuff', ',', 'innit', '?', 'I', \"'m\", 'like', ',', 'totally', 'loving', 'it', '!']\n",
            "Words after Stop Word Removal: ['OMG', 'NLTK', 'pretty', 'cool', 'NLP', 'stuff', 'innit', 'like', 'totally', 'loving']\n",
            "Words after Stemming: ['omg', 'nltk', 'pretti', 'cool', 'nlp', 'stuff', 'innit', 'like', 'total', 'love']\n",
            "Words after Lemmatization (with POS): ['OMG', 'NLTK', 'pretty', 'cool', 'NLP', 'stuff', 'innit', 'like', 'totally', 'love']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35715c97"
      },
      "source": [
        "### Summary of NLTK Text Preprocessing Tasks and Outcomes\n",
        "\n",
        "We have successfully performed a series of NLTK text preprocessing tasks:\n",
        "\n",
        "1.  **NLTK Data Download**: Successfully downloaded essential NLTK packages including 'punkt' (for tokenization), 'stopwords' (for stop word removal), 'wordnet' (for lemmatization), and 'averaged_perceptron_tagger_eng' (for POS tagging required for accurate lemmatization).\n",
        "\n",
        "2.  **Word Tokenization**: Applied `nltk.word_tokenize` to a sample sentence, breaking it down into individual words and punctuation marks.\n",
        "    *   *Example Outcome*: `['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']`\n",
        "\n",
        "3.  **Stop Word Removal**: Filtered out common English stop words from the tokenized words using `nltk.corpus.stopwords`, focusing on content-bearing terms.\n",
        "    *   *Example Outcome*: `['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']`\n",
        "\n",
        "4.  **Stemming**: Applied `PorterStemmer` to the filtered words, reducing words to their root or base form, often an incomplete word. This is a heuristic process.\n",
        "    *   *Example Outcome*: `['nltk', 'power', 'librari', 'natur', 'languag', 'process']`\n",
        "\n",
        "5.  **Lemmatization**: Performed lemmatization using `WordNetLemmatizer`.\n",
        "    *   **Default Lemmatization**: Initially, without POS tagging, it often returned the word itself if its default (noun) lemma was the same.\n",
        "    *   **POS-aware Lemmatization**: By integrating `nltk.pos_tag` and mapping to WordNet POS tags, we achieved more accurate lemmatization, especially for verbs and adjectives. For instance, 'running' became 'run' when treated as a verb.\n",
        "\n",
        "6.  **Comparison of Stemming and Lemmatization**: Demonstrated the differences between stemming (e.g., 'studies' -> 'studi') and POS-aware lemmatization (e.g., 'studies' -> 'study', 'better' -> 'well', 'running' -> 'run'). Stemming is a cruder process, while lemmatization aims for linguistically correct base forms.\n",
        "\n",
        "7.  **Sentence Tokenization**: Used `nltk.sent_tokenize` to split a paragraph into a list of complete sentences.\n",
        "    *   *Example Outcome*: `['Natural Language Processing (NLP) is a field of artificial intelligence.', 'It focuses on the interaction between computers and human language.', 'NLTK is a popular library for NLP tasks.']`\n",
        "\n",
        "8.  **Comprehensive Preprocessing on Informal Text**: Applied the full pipeline (tokenization, stop word removal, stemming, and POS-aware lemmatization) to an informal sentence, showcasing how these techniques handle colloquial language and normalize it.\n",
        "    *   *Example Informal Text Outcome (Lemmatized)*: `['OMG', 'NLTK', 'pretty', 'cool', 'NLP', 'stuff', 'innit', 'like', 'totally', 'love']`\n",
        "\n",
        "These steps collectively demonstrate the fundamental text preprocessing capabilities of NLTK, crucial for preparing text data for various NLP applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6286f4"
      },
      "source": [
        "### Summary of NLTK Text Preprocessing Tasks and Outcomes\n",
        "\n",
        "We have successfully performed a series of NLTK text preprocessing tasks:\n",
        "\n",
        "1.  **NLTK Data Download**: Successfully downloaded essential NLTK packages including 'punkt' (for tokenization), 'stopwords' (for stop word removal), 'wordnet' (for lemmatization), and 'averaged_perceptron_tagger_eng' (for POS tagging required for accurate lemmatization).\n",
        "\n",
        "2.  **Word Tokenization**: Applied `nltk.word_tokenize` to a sample sentence, breaking it down into individual words and punctuation marks.\n",
        "    *   *Example Outcome*: `['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.']`\n",
        "\n",
        "3.  **Stop Word Removal**: Filtered out common English stop words from the tokenized words using `nltk.corpus.stopwords`, focusing on content-bearing terms.\n",
        "    *   *Example Outcome*: `['NLTK', 'powerful', 'library', 'natural', 'language', 'processing']`\n",
        "\n",
        "4.  **Stemming**: Applied `PorterStemmer` to the filtered words, reducing words to their root or base form, often an incomplete word. This is a heuristic process.\n",
        "    *   *Example Outcome*: `['nltk', 'power', 'librari', 'natur', 'languag', 'process']`\n",
        "\n",
        "5.  **Lemmatization**: Performed lemmatization using `WordNetLemmatizer`.\n",
        "    *   **Default Lemmatization**: Initially, without POS tagging, it often returned the word itself if its default (noun) lemma was the same.\n",
        "    *   **POS-aware Lemmatization**: By integrating `nltk.pos_tag` and mapping to WordNet POS tags, we achieved more accurate lemmatization, especially for verbs and adjectives. For instance, 'running' became 'run' when treated as a verb.\n",
        "\n",
        "6.  **Comparison of Stemming and Lemmatization**: Demonstrated the differences between stemming (e.g., 'studies' -> 'studi') and POS-aware lemmatization (e.g., 'studies' -> 'study', 'better' -> 'well', 'running' -> 'run'). Stemming is a cruder process, while lemmatization aims for linguistically correct base forms.\n",
        "\n",
        "7.  **Sentence Tokenization**: Used `nltk.sent_tokenize` to split a paragraph into a list of complete sentences.\n",
        "    *   *Example Outcome*: `['Natural Language Processing (NLP) is a field of artificial intelligence.', 'It focuses on the interaction between computers and human language.', 'NLTK is a popular library for NLP tasks.']`\n",
        "\n",
        "8.  **Comprehensive Preprocessing on Informal Text**: Applied the full pipeline (tokenization, stop word removal, stemming, and POS-aware lemmatization) to an informal sentence, showcasing how these techniques handle colloquial language and normalize it.\n",
        "    *   *Example Informal Text Outcome (Lemmatized)*: `['OMG', 'NLTK', 'pretty', 'cool', 'NLP', 'stuff', 'innit', 'like', 'totally', 'love']`\n",
        "\n",
        "These steps collectively demonstrate the fundamental text preprocessing capabilities of NLTK, crucial for preparing text data for various NLP applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26257b7c"
      },
      "source": [
        "## Word Tokenization (Task 1)\n",
        "\n",
        "### Subtask:\n",
        "Apply word tokenization using NLTK on the sentence: 'Generative AI models are transforming content creation platforms.' and display the tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac630062"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform word tokenization as per the subtask, I will define the given sentence, use `nltk.word_tokenize()` to break it into words, and then print both the original sentence and the tokenized output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8aa6acd",
        "outputId": "b201258d-6b6c-4748-f3ec-46c868013068"
      },
      "source": [
        "import nltk\n",
        "\n",
        "text_task1 = 'Generative AI models are transforming content creation platforms.'\n",
        "word_tokens_task1 = nltk.word_tokenize(text_task1)\n",
        "\n",
        "print(f\"Original Sentence: {text_task1}\")\n",
        "print(f\"Word Tokens: {word_tokens_task1}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Generative AI models are transforming content creation platforms.\n",
            "Word Tokens: ['Generative', 'AI', 'models', 'are', 'transforming', 'content', 'creation', 'platforms', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e04523"
      },
      "source": [
        "## Stop Word Removal (Task 2)\n",
        "\n",
        "### Subtask:\n",
        "Perform stop word removal using NLTK on the sentence: 'Smart cities are using IOT sensors for real time monitoring .' and display the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ac2ad6"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform stop word removal, I will first define the target sentence, tokenize it, then load the English stop words, and finally filter the tokenized words to exclude stop words and non-alphabetic tokens, displaying both the original sentence and the processed output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4da5c683",
        "outputId": "627c2bba-d290-4370-c0cc-9f0b465d3d26"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "text_task2 = 'Smart cities are using IOT sensors for real time monitoring .'\n",
        "word_tokens_task2 = nltk.word_tokenize(text_task2)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words_task2 = [word for word in word_tokens_task2 if word.lower() not in stop_words and word.isalpha()]\n",
        "\n",
        "print(f\"Original Sentence: {text_task2}\")\n",
        "print(f\"Words after Stop Word Removal: {filtered_words_task2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Smart cities are using IOT sensors for real time monitoring .\n",
            "Words after Stop Word Removal: ['Smart', 'cities', 'using', 'IOT', 'sensors', 'real', 'time', 'monitoring']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6490889a"
      },
      "source": [
        "## Stemming (Task 3)\n",
        "\n",
        "### Subtask:\n",
        "Apply stemming using NLTK (e.g., PorterStemmer) to the words: 'processing', 'processed', 'processes', 'processor' and display the stemmed forms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0473b060"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform stemming as per the subtask, I will first define the list of words, then import and initialize the `PorterStemmer`, and finally iterate through the words to apply the stemmer and display both the original and stemmed forms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9abbe6a0"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, I will define the list of words, import `PorterStemmer`, create an instance, and then loop through the words to apply stemming and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8b94112",
        "outputId": "689328f0-4daf-46c7-edaf-c79b88c7f337"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "words_for_stemming = ['processing', 'processed', 'processes', 'processor']\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"Stemming Results:\")\n",
        "for word in words_for_stemming:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    print(f\"  Original: {word: <12} | Stemmed: {stemmed_word}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming Results:\n",
            "  Original: processing   | Stemmed: process\n",
            "  Original: processed    | Stemmed: process\n",
            "  Original: processes    | Stemmed: process\n",
            "  Original: processor    | Stemmed: processor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "503c3b94"
      },
      "source": [
        "## Lemmatization (Task 4)\n",
        "\n",
        "### Subtask:\n",
        "Apply lemmatization using NLTK (WordNetLemmatizer) to the words: 'models', 'data', 'running', 'better' and display the lemmas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b309ad6"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply lemmatization with Part-of-Speech (POS) tagging as requested, I will import `WordNetLemmatizer` and `wordnet`, define the list of words, create a helper function to map NLTK POS tags to WordNet POS tags, and then iterate through the words to lemmatize and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d56dec46",
        "outputId": "567b8ff1-be25-400b-e09a-c97c7107e62b"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure 'averaged_perceptron_tagger_eng' is downloaded for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "words_for_lemmatization = ['models', 'data', 'running', 'better']\n",
        "\n",
        "print(\"Lemmatization Results:\")\n",
        "for word in words_for_lemmatization:\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "    print(f\"  Original: {word: <10} | Lemmatized: {lemmatized_word}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization Results:\n",
            "  Original: models     | Lemmatized: model\n",
            "  Original: data       | Lemmatized: data\n",
            "  Original: running    | Lemmatized: run\n",
            "  Original: better     | Lemmatized: well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2db387e"
      },
      "source": [
        "## Tokenization & Stop Word Removal (Task 5)\n",
        "\n",
        "### Subtask:\n",
        "Perform word tokenization and then stop word removal using NLTK on the sentence: 'Mobile applications improve user engagement!' and display the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea849f5c"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform word tokenization and stop word removal as per the subtask, I will first define the target sentence, then tokenize it using `nltk.word_tokenize`. Next, I will load the English stop words from `nltk.corpus.stopwords` and filter the tokenized words, ensuring only alphabetic words that are not stop words remain. Finally, I will display both the original sentence and the words after stop word removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03719698"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the reasoning, I will implement the code to perform word tokenization and stop word removal on the given sentence, then print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6551b918",
        "outputId": "446ac96d-59dd-471a-bfb7-549ff30eb4d3"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text_task5 = 'Mobile applications improve user engagement!'\n",
        "\n",
        "# 1. Word Tokenization\n",
        "word_tokens_task5 = nltk.word_tokenize(text_task5)\n",
        "\n",
        "# 2. Get English stop words\n",
        "stop_words_task5 = set(stopwords.words('english'))\n",
        "\n",
        "# 3. Filter out stop words and non-alphabetic tokens\n",
        "filtered_words_task5 = [word for word in word_tokens_task5 if word.lower() not in stop_words_task5 and word.isalpha()]\n",
        "\n",
        "print(f\"Original Sentence: {text_task5}\")\n",
        "print(f\"Word Tokens: {word_tokens_task5}\")\n",
        "print(f\"Words after Stop Word Removal: {filtered_words_task5}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Mobile applications improve user engagement!\n",
            "Word Tokens: ['Mobile', 'applications', 'improve', 'user', 'engagement', '!']\n",
            "Words after Stop Word Removal: ['Mobile', 'applications', 'improve', 'user', 'engagement']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8165b5"
      },
      "source": [
        "## Stemming & Lemmatization (Task 6)\n",
        "\n",
        "### Subtask:\n",
        "Apply both stemming and lemmatization using NLTK on the sentence: 'Autonomous drones were capturing multispectral image continuously.' and compare their outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52996f64"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform both stemming and lemmatization and compare their outputs, I need to first tokenize the given sentence. Then, I will initialize `PorterStemmer` and `WordNetLemmatizer` objects. A helper function to map NLTK POS tags to WordNet POS tags will be crucial for accurate lemmatization. Finally, I will iterate through each token, apply both stemming and POS-aware lemmatization, and print the results side-by-side for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4199c29"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will implement the code to perform word tokenization, then apply both stemming and POS-aware lemmatization to each word from the specified sentence, and finally display a comparison of their outputs as detailed in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74dfed02",
        "outputId": "4407ff9e-aa31-4307-c528-3d72098b54bc"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure necessary downloads for this comparison\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "\n",
        "sentence_task6 = 'Autonomous drones were capturing multispectral image continuously.'\n",
        "\n",
        "# 1. Word Tokenization\n",
        "words_task6 = nltk.word_tokenize(sentence_task6)\n",
        "\n",
        "# 2. Initialize Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 3. Helper function for POS tagging\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "print(f\"Original Sentence: {sentence_task6}\")\n",
        "print(\"\\nComparison of Stemming and Lemmatization:\")\n",
        "print(f\"{'Word': <15} | {'Stemmed': <15} | {'Lemmatized (POS)': <20}\")\n",
        "print(f\"{\"-\"*15} | {\"-\"*15} | {\"-\"*20}\")\n",
        "\n",
        "# 4. Iterate and compare\n",
        "for word in words_task6:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "    print(f\"{word: <15} | {stemmed_word: <15} | {lemmatized_word: <20}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Autonomous drones were capturing multispectral image continuously.\n",
            "\n",
            "Comparison of Stemming and Lemmatization:\n",
            "Word            | Stemmed         | Lemmatized (POS)    \n",
            "--------------- | --------------- | --------------------\n",
            "Autonomous      | autonom         | Autonomous          \n",
            "drones          | drone           | drone               \n",
            "were            | were            | be                  \n",
            "capturing       | captur          | capture             \n",
            "multispectral   | multispectr     | multispectral       \n",
            "image           | imag            | image               \n",
            "continuously    | continu         | continuously        \n",
            ".               | .               | .                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "146aae1f"
      },
      "source": [
        "## Full Preprocessing (Task 7)\n",
        "\n",
        "### Subtask:\n",
        "Perform word tokenization, stop word removal, stemming, and lemmatization using NLTK on the sentence: 'AI-powered chatbots are assisting users in digital healthcare systems.' and display the step-by-step results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7da841"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform the full preprocessing pipeline on the given sentence, I will define the sentence, apply word tokenization, then remove stop words, followed by stemming, and finally, POS-aware lemmatization, displaying the results of each step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78c42c73",
        "outputId": "0ca63d6f-b906-4d13-8d56-31624f78612d"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary downloads for POS tagging\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "sentence_task7 = 'AI-powered chatbots are assisting users in digital healthcare systems.'\n",
        "\n",
        "print(f\"Original Sentence: {sentence_task7}\")\n",
        "\n",
        "# 1. Word Tokenization\n",
        "word_tokens_task7 = nltk.word_tokenize(sentence_task7)\n",
        "print(f\"\\n1. Word Tokenization: {word_tokens_task7}\")\n",
        "\n",
        "# 2. Stop Word Removal\n",
        "stop_words_task7 = set(stopwords.words('english'))\n",
        "filtered_words_task7 = [word for word in word_tokens_task7 if word.lower() not in stop_words_task7 and word.isalpha()]\n",
        "print(f\"\\n2. Words after Stop Word Removal: {filtered_words_task7}\")\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words_task7 = [stemmer.stem(word) for word in filtered_words_task7]\n",
        "print(f\"\\n3. Words after Stemming: {stemmed_words_task7}\")\n",
        "\n",
        "# 4. Lemmatization (with POS-tagging)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized_words_task7 = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words_task7]\n",
        "print(f\"\\n4. Words after Lemmatization (with POS): {lemmatized_words_task7}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: AI-powered chatbots are assisting users in digital healthcare systems.\n",
            "\n",
            "1. Word Tokenization: ['AI-powered', 'chatbots', 'are', 'assisting', 'users', 'in', 'digital', 'healthcare', 'systems', '.']\n",
            "\n",
            "2. Words after Stop Word Removal: ['chatbots', 'assisting', 'users', 'digital', 'healthcare', 'systems']\n",
            "\n",
            "3. Words after Stemming: ['chatbot', 'assist', 'user', 'digit', 'healthcar', 'system']\n",
            "\n",
            "4. Words after Lemmatization (with POS): ['chatbots', 'assist', 'user', 'digital', 'healthcare', 'system']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39c37340"
      },
      "source": [
        "## Word Tokenization on Paragraph (Task 8)\n",
        "\n",
        "### Subtask:\n",
        "Apply word tokenization using NLTK on a three-line paragraph related to climate change analytics (example text to be provided) and display the tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fe33f47"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform word tokenization as per the subtask, I will first define the specified three-line paragraph related to climate change analytics. Then, I will import the `nltk` library and use `nltk.word_tokenize()` to break the paragraph into individual words. Finally, I will print both the original paragraph and the resulting list of word tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df9f7f26"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I will implement the code to define the paragraph, apply word tokenization using `nltk.word_tokenize()`, and print the original paragraph and the resulting tokens, as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd8ce59a",
        "outputId": "63ed068c-46c4-4771-e169-856b4c79c856"
      },
      "source": [
        "import nltk\n",
        "\n",
        "paragraph_task8 = 'Climate change analytics leverages vast datasets. These datasets include historical weather patterns, satellite imagery, and environmental sensor readings. The insights gained are crucial for policy-making and mitigation strategies.'\n",
        "\n",
        "word_tokens_task8 = nltk.word_tokenize(paragraph_task8)\n",
        "\n",
        "print(f\"Original Paragraph:\\n{paragraph_task8}\")\n",
        "print(f\"\\nWord Tokens: {word_tokens_task8}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "Climate change analytics leverages vast datasets. These datasets include historical weather patterns, satellite imagery, and environmental sensor readings. The insights gained are crucial for policy-making and mitigation strategies.\n",
            "\n",
            "Word Tokens: ['Climate', 'change', 'analytics', 'leverages', 'vast', 'datasets', '.', 'These', 'datasets', 'include', 'historical', 'weather', 'patterns', ',', 'satellite', 'imagery', ',', 'and', 'environmental', 'sensor', 'readings', '.', 'The', 'insights', 'gained', 'are', 'crucial', 'for', 'policy-making', 'and', 'mitigation', 'strategies', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfff7869"
      },
      "source": [
        "## Stop Word Removal on Text (Task 9)\n",
        "\n",
        "### Subtask:\n",
        "Perform stop word removal using NLTK on a two-line text related to social media posts about sustainability (example text to be provided) and display the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65871fb7"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform stop word removal as per the subtask, I will define the two-line text, tokenize it, load the English stop words, and then filter the tokenized words to remove stop words and non-alphabetic tokens. Finally, I will display both the original text and the processed output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4085bbc0",
        "outputId": "989ecb64-2a18-4ff6-c64d-6ac2d34c04c2"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text_task9 = 'Green initiatives are trending on social media. Many users advocate for sustainable living practices.'\n",
        "\n",
        "# 1. Word Tokenization\n",
        "word_tokens_task9 = nltk.word_tokenize(text_task9)\n",
        "\n",
        "# 2. Load English stop words\n",
        "stop_words_task9 = set(stopwords.words('english'))\n",
        "\n",
        "# 3. Filter out stop words and non-alphabetic tokens\n",
        "filtered_words_task9 = [word for word in word_tokens_task9 if word.lower() not in stop_words_task9 and word.isalpha()]\n",
        "\n",
        "print(f\"Original Text:\\n{text_task9}\")\n",
        "print(f\"\\nWords after Stop Word Removal: {filtered_words_task9}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Green initiatives are trending on social media. Many users advocate for sustainable living practices.\n",
            "\n",
            "Words after Stop Word Removal: ['Green', 'initiatives', 'trending', 'social', 'media', 'Many', 'users', 'advocate', 'sustainable', 'living', 'practices']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e012ec76"
      },
      "source": [
        "## Stemming & Lemmatization on Text (Task 10)\n",
        "\n",
        "### Subtask:\n",
        "Apply both stemming and lemmatization using NLTK on a two-line text related to healthcare application (example text to be provided) and compare their outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014e8e16"
      },
      "source": [
        "**Reasoning**:\n",
        "To apply both stemming and lemmatization and compare their outputs on the new healthcare-related text, I will first tokenize the text. Then, I will initialize the stemmer and lemmatizer, use the helper function for POS tagging, and iterate through the words to print a side-by-side comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ecb744",
        "outputId": "53013643-4f45-482a-fbe3-d2d15126c44a"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure necessary downloads for POS tagging (if not already done)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "\n",
        "text_task10 = 'Modern healthcare systems are rapidly adopting AI solutions. These technologies are improving diagnostics and patient care outcomes.'\n",
        "\n",
        "# 1. Word Tokenization\n",
        "words_task10 = nltk.word_tokenize(text_task10)\n",
        "\n",
        "# 2. Initialize Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 3. Helper function for POS tagging\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "print(f\"Original Text:\\n{text_task10}\")\n",
        "print(\"\\nComparison of Stemming and Lemmatization:\")\n",
        "print(f\"{'Word': <20} | {'Stemmed': <20} | {'Lemmatized (POS)': <25}\")\n",
        "print(f\"{\"-\"*20} | {\"-\"*20} | {\"-\"*25}\")\n",
        "\n",
        "# 4. Iterate and compare\n",
        "for word in words_task10:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "    print(f\"{word: <20} | {stemmed_word: <20} | {lemmatized_word: <25}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Modern healthcare systems are rapidly adopting AI solutions. These technologies are improving diagnostics and patient care outcomes.\n",
            "\n",
            "Comparison of Stemming and Lemmatization:\n",
            "Word                 | Stemmed              | Lemmatized (POS)         \n",
            "-------------------- | -------------------- | -------------------------\n",
            "Modern               | modern               | Modern                   \n",
            "healthcare           | healthcar            | healthcare               \n",
            "systems              | system               | system                   \n",
            "are                  | are                  | be                       \n",
            "rapidly              | rapidli              | rapidly                  \n",
            "adopting             | adopt                | adopt                    \n",
            "AI                   | ai                   | AI                       \n",
            "solutions            | solut                | solution                 \n",
            ".                    | .                    | .                        \n",
            "These                | these                | These                    \n",
            "technologies         | technolog            | technology               \n",
            "are                  | are                  | be                       \n",
            "improving            | improv               | improve                  \n",
            "diagnostics          | diagnost             | diagnostics              \n",
            "and                  | and                  | and                      \n",
            "patient              | patient              | patient                  \n",
            "care                 | care                 | care                     \n",
            "outcomes             | outcom               | outcome                  \n",
            ".                    | .                    | .                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5c3b657"
      },
      "source": [
        "## Compare Stemming & Lemmatization (Task 11)\n",
        "\n",
        "### Subtask:\n",
        "Compare the outputs of stemming and lemmatization using NLTK on the words: 'learning', 'learned', 'learns', 'learner' and display the comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a11b985"
      },
      "source": [
        "**Reasoning**:\n",
        "To compare stemming and lemmatization, I will implement the code as described in the instructions, which includes importing necessary NLTK modules, downloading required data, defining the words, initializing the stemmer and lemmatizer, creating a POS mapping function, and then iterating to display the stemmed and lemmatized forms of each word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69588943",
        "outputId": "c798326e-4980-49a4-8301-d3c96a87680b"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure necessary downloads for this comparison\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "\n",
        "# Define a list of words for comparison\n",
        "words_for_comparison_task11 = ['learning', 'learned', 'learns', 'learner']\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "print(\"Comparison of Stemming and Lemmatization (Task 11):\")\n",
        "print(f\"{'Word': <15} | {'Stemmed': <15} | {'Lemmatized (POS)': <20}\")\n",
        "print(f\"{\"-\"*15} | {\"-\"*15} | {\"-\"*20}\")\n",
        "\n",
        "for word in words_for_comparison_task11:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
        "    print(f\"{word: <15} | {stemmed_word: <15} | {lemmatized_word: <20}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Stemming and Lemmatization (Task 11):\n",
            "Word            | Stemmed         | Lemmatized (POS)    \n",
            "--------------- | --------------- | --------------------\n",
            "learning        | learn           | learn               \n",
            "learned         | learn           | learn               \n",
            "learns          | learn           | learns              \n",
            "learner         | learner         | learner             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00dd9a18"
      },
      "source": [
        "## Sentence Tokenization (Task 12)\n",
        "\n",
        "### Subtask:\n",
        "Write a program using NLTK to perform sentence tokenization on a three-line paragraph related to industry 4.0 (example text to be provided) and display the tokenized sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b560584"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I will implement the code to define the paragraph, apply sentence tokenization using `nltk.sent_tokenize()`, and print the original paragraph and the resulting tokens, as outlined in the instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ba4ea63"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will write the Python code to define the specified three-line paragraph, apply `nltk.sent_tokenize()` to it, and then print both the original paragraph and the resulting list of tokenized sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e110638",
        "outputId": "fce1edef-9371-4d16-e53c-c9f9ae72d6fb"
      },
      "source": [
        "import nltk\n",
        "\n",
        "paragraph_task12 = 'Industry 4.0 is revolutionizing manufacturing processes. It integrates advanced technologies like AI, IoT, and robotics. This leads to smart factories and enhanced productivity.'\n",
        "\n",
        "sentences_task12 = nltk.sent_tokenize(paragraph_task12)\n",
        "\n",
        "print(f\"Original Paragraph:\\n{paragraph_task12}\")\n",
        "print(f\"\\nSentence Tokens: {sentences_task12}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            "Industry 4.0 is revolutionizing manufacturing processes. It integrates advanced technologies like AI, IoT, and robotics. This leads to smart factories and enhanced productivity.\n",
            "\n",
            "Sentence Tokens: ['Industry 4.0 is revolutionizing manufacturing processes.', 'It integrates advanced technologies like AI, IoT, and robotics.', 'This leads to smart factories and enhanced productivity.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b67229"
      },
      "source": [
        "## Word Tokenization & Stop Word Removal (Task 13)\n",
        "\n",
        "### Subtask:\n",
        "Apply word tokenization and stop word removal using NLTK on the sentence: 'Digital twins enable predictive maintenance in smart manufacturing!' and display the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeb9d768"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform word tokenization and stop word removal as per the subtask, I will first define the target sentence, then tokenize it using `nltk.word_tokenize`. Next, I will load the English stop words from `nltk.corpus.stopwords` and filter the tokenized words, ensuring only alphabetic words that are not stop words remain. Finally, I will display both the original sentence and the words after stop word removal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ade725"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the reasoning, I will implement the code to perform word tokenization and stop word removal on the given sentence, then print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0baa019f",
        "outputId": "fbea1bde-366b-483f-c64e-007f4ed48aa2"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text_task13 = 'Digital twins enable predictive maintenance in smart manufacturing!'\n",
        "\n",
        "# 1. Word Tokenization\n",
        "word_tokens_task13 = nltk.word_tokenize(text_task13)\n",
        "\n",
        "# 2. Get English stop words\n",
        "stop_words_task13 = set(stopwords.words('english'))\n",
        "\n",
        "# 3. Filter out stop words and non-alphabetic tokens\n",
        "filtered_words_task13 = [word for word in word_tokens_task13 if word.lower() not in stop_words_task13 and word.isalpha()]\n",
        "\n",
        "print(f\"Original Sentence: {text_task13}\")\n",
        "print(f\"Word Tokens: {word_tokens_task13}\")\n",
        "print(f\"Words after Stop Word Removal: {filtered_words_task13}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Digital twins enable predictive maintenance in smart manufacturing!\n",
            "Word Tokens: ['Digital', 'twins', 'enable', 'predictive', 'maintenance', 'in', 'smart', 'manufacturing', '!']\n",
            "Words after Stop Word Removal: ['Digital', 'twins', 'enable', 'predictive', 'maintenance', 'smart', 'manufacturing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "391ff916"
      },
      "source": [
        "## Text Preprocessing on Informal Text (Task 14)\n",
        "\n",
        "### Subtask:\n",
        "Perform comprehensive text preprocessing (word tokenization, stop word removal, stemming, lemmatization) using NLTK on an informal two-line text related to an online user review of AI tools (example text to be provided) and display the processed text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b73c32fc"
      },
      "source": [
        "**Reasoning**:\n",
        "To perform the comprehensive text preprocessing, I will first define the informal two-line text as instructed. Then, I will sequentially apply word tokenization, stop word removal, stemming, and lemmatization (with POS-tagging). Each step will build upon the previous one, and the results of the initial and final steps will be displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1505e107"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I will implement the code to perform word tokenization, stop word removal, stemming, and POS-aware lemmatization on the given informal text, displaying the original text and the final lemmatized output as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39521f1e",
        "outputId": "81a996b0-457c-40be-87bf-9f62fa1eb580"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary downloads for POS tagging (if not already done)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "informal_text_task14 = \"OMG, this AI tool is totally insane! It's so good, like, seriously improved my workflow big time!!\"\n",
        "\n",
        "print(f\"Original Informal Text:\\n{informal_text_task14}\")\n",
        "\n",
        "# 1. Word Tokenization\n",
        "word_tokens_task14 = nltk.word_tokenize(informal_text_task14)\n",
        "\n",
        "# 2. Stop Word Removal\n",
        "stop_words_task14 = set(stopwords.words('english'))\n",
        "# Filter out stop words, non-alphabetic tokens, and punctuation\n",
        "filtered_words_task14 = [word for word in word_tokens_task14 if word.lower() not in stop_words_task14 and word.isalpha()]\n",
        "\n",
        "# 3. Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words_task14 = [stemmer.stem(word) for word in filtered_words_task14]\n",
        "\n",
        "# 4. Lemmatization (with POS-tagging)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "lemmatized_words_task14 = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in filtered_words_task14]\n",
        "\n",
        "print(f\"\\nProcessed Text (Lemmatized with POS): {lemmatized_words_task14}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Informal Text:\n",
            "OMG, this AI tool is totally insane! It's so good, like, seriously improved my workflow big time!!\n",
            "\n",
            "Processed Text (Lemmatized with POS): ['OMG', 'AI', 'tool', 'totally', 'insane', 'good', 'like', 'seriously', 'improve', 'workflow', 'big', 'time']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189c401d"
      },
      "source": [
        "## Lemmatization Only (Task 15)\n",
        "\n",
        "### Subtask:\n",
        "Apply lemmatization only using NLTK on the sentence: 'Farmers are using AI-driven advisory systems for client-resilient agriculture' and display the lemmatized output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e24be39f"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I will implement the code to define the sentence, apply word tokenization, and then perform POS-aware lemmatization on the tokenized words, displaying the original sentence and the final lemmatized output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cd4e82a"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will write the Python code to define the specified sentence, apply word tokenization, and then perform POS-aware lemmatization on the tokenized words, displaying the original sentence and the final lemmatized output, as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d9a7ab3",
        "outputId": "0b000d6d-489f-4295-8f12-0ee700606ee0"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Ensure necessary downloads for POS tagging (if not already done)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "sentence_task15 = 'Farmers are using AI-driven advisory systems for client-resilient agriculture'\n",
        "\n",
        "# 1. Word Tokenization\n",
        "word_tokens_task15 = nltk.word_tokenize(sentence_task15)\n",
        "\n",
        "# 2. Initialize Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 3. Helper function for POS tagging\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# 4. Lemmatize words with POS tagging\n",
        "lemmatized_words_task15 = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_tokens_task15]\n",
        "\n",
        "print(f\"Original Sentence: {sentence_task15}\")\n",
        "print(f\"Lemmatized Output: {lemmatized_words_task15}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: Farmers are using AI-driven advisory systems for client-resilient agriculture\n",
            "Lemmatized Output: ['Farmers', 'be', 'use', 'AI-driven', 'advisory', 'system', 'for', 'client-resilient', 'agriculture']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15916c66"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of all the NLTK text preprocessing tasks performed and their outcomes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78c5b2eb"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **NLTK Data Dependency**: Various NLTK preprocessing tasks require specific data downloads, including 'punkt' for tokenization, 'stopwords' for stop word removal, 'wordnet' for lemmatization, and 'averaged\\_perceptron\\_tagger\\_eng' for accurate Part-of-Speech (POS) tagging during lemmatization. These were successfully downloaded as needed.\n",
        "*   **Word Tokenization**: `nltk.word_tokenize` successfully breaks down sentences and paragraphs into individual words and punctuation, as demonstrated with sentences like 'Generative AI models are transforming content creation platforms.' which yielded tokens such as `['Generative', 'AI', 'models', 'are', 'transforming', 'content', 'creation', 'platforms', '.']`.\n",
        "*   **Stop Word Removal**: This process effectively filters out common English stop words and non-alphabetic characters. For instance, from 'Smart cities are using IOT sensors for real time monitoring.', the processed output was `['Smart', 'cities', 'using', 'IOT', 'sensors', 'real', 'time', 'monitoring']`.\n",
        "*   **Stemming Characteristics**: `PorterStemmer` reduces words to their root form, which can sometimes result in truncated or non-dictionary words. For example, 'processing', 'processed', and 'processes' all stemmed to 'process', while 'capturing' stemmed to 'captur' and 'continuously' to 'continu'.\n",
        "*   **Lemmatization Accuracy (with POS Tagging)**: `WordNetLemmatizer`, when used with POS tagging (via `get_wordnet_pos` helper function mapping NLTK POS tags to WordNet tags), provides linguistically accurate base forms (lemmas). Examples include 'running' becoming 'run', 'better' becoming 'well', 'are' becoming 'be', 'systems' becoming 'system', and 'assisting' becoming 'assist'.\n",
        "*   **Stemming vs. Lemmatization Comparison**: The analysis consistently highlighted that stemming is a faster, heuristic approach that may produce non-dictionary words, whereas POS-aware lemmatization is a more sophisticated, linguistically informed process that yields valid dictionary words, generally providing a more meaningful base form. For the word 'learns', stemming resulted in 'learn', while POS-aware lemmatization also produced 'learns' (indicating it was its base form in context).\n",
        "*   **Sentence Tokenization**: `nltk.sent_tokenize` accurately segments paragraphs into a list of complete sentences, as shown when a paragraph on Industry 4.0 was split into `['Industry 4.0 is revolutionizing manufacturing processes.', 'It integrates advanced technologies like AI, IoT, and robotics.', 'This leads to smart factories and enhanced productivity.']`.\n",
        "*   **Comprehensive Preprocessing on Informal Text**: The full pipeline (tokenization, stop word removal, stemming, and POS-aware lemmatization) successfully normalized informal text. For example, the informal text \"OMG, this AI tool is totally insane! It's so good, like, seriously improved my workflow big time!!\" was processed (lemmatized) into `['OMG', 'AI', 'tool', 'totally', 'insane', 'good', 'like', 'seriously', 'improve', 'workflow', 'big', 'time']`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Choice of Preprocessing Technique**: The choice between stemming and lemmatization should be driven by the specific NLP task. For tasks requiring linguistic accuracy and dictionary words (e.g., semantic analysis, machine translation), lemmatization with POS tagging is generally preferred. For tasks where speed and a common root are more important and the exact word form is less critical (e.g., information retrieval), stemming can be sufficient.\n",
        "*   **Robustness of NLTK**: NLTK provides a robust set of tools for fundamental text preprocessing, effectively handling various text types, including informal language, and preparing them for further NLP analysis. Future steps could involve exploring more advanced preprocessing techniques, such as handling contractions, emoticons, or domain-specific terminology, which might require custom rules or more specialized libraries.\n"
      ]
    }
  ]
}