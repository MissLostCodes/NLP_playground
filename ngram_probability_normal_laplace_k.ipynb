{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# No Laplace ngram"
      ],
      "metadata": {
        "id": "A_YqSeIL-RYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_probability(corpus, sentence):\n",
        "    list_len = len(corpus)\n",
        "    print(f\"Corpus length: {list_len}\")\n",
        "    print(\"===== STEP 1: TOKENIZE CORPUS =====\")\n",
        "    tokenized_corpus = []\n",
        "    for sent in corpus:\n",
        "        tokens = sent.lower().split()\n",
        "        tokenized_corpus.append(tokens)\n",
        "        print(f\"Sentence: '{sent}' -> Tokens: {tokens}\")\n",
        "\n",
        "    print(\"\\n===== STEP 2: COUNT UNIGRAMS =====\")\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sent in tokenized_corpus:\n",
        "        for word in sent:\n",
        "            unigram_counts[word] = unigram_counts.get(word, 0) + 1\n",
        "            total_unigrams += 1\n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        print(f\"Unigram '{word}': {count}\")\n",
        "\n",
        "    print(f\"Total unigrams in corpus: {total_unigrams}\")\n",
        "\n",
        "    print(\"\\n===== STEP 3: COUNT BIGRAMS =====\")\n",
        "    bigram_counts = {}\n",
        "\n",
        "    for sent in tokenized_corpus:\n",
        "        for i in range(len(sent) - 1):\n",
        "            bigram = (sent[i], sent[i + 1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        print(f\"Bigram {bigram}: {count}\")\n",
        "\n",
        "    print(\"\\n===== STEP 4: TOKENIZE INPUT SENTENCE =====\")\n",
        "    sentence_tokens = sentence.lower().split()\n",
        "    print(f\"Input sentence tokens: {sentence_tokens}\")\n",
        "\n",
        "    print(\"\\n===== STEP 5: CALCULATE PROBABILITY =====\")\n",
        "\n",
        "    probability = 1.0\n",
        "\n",
        "    # P(w1)\n",
        "    first_word = sentence_tokens[0]\n",
        "    if first_word not in unigram_counts:\n",
        "        print(f\"P({first_word}) = 0 (word not in corpus)\")\n",
        "        return 0.0\n",
        "\n",
        "    p_first = unigram_counts[first_word] / list_len\n",
        "    probability *= p_first\n",
        "\n",
        "    print(f\"P({first_word}) = {unigram_counts[first_word]} / {list_len} = {p_first}\")\n",
        "\n",
        "    # Conditional probabilities\n",
        "    for i in range(len(sentence_tokens) - 1):\n",
        "        prev_word = sentence_tokens[i]\n",
        "        curr_word = sentence_tokens[i + 1]\n",
        "        bigram = (prev_word, curr_word)\n",
        "\n",
        "        if bigram not in bigram_counts:\n",
        "            print(f\"P({curr_word} | {prev_word}) = 0 (bigram not found)\")\n",
        "            return 0.0\n",
        "\n",
        "        p_conditional = bigram_counts[bigram] / unigram_counts[prev_word]\n",
        "        probability *= p_conditional\n",
        "\n",
        "        print(\n",
        "            f\"P({curr_word} | {prev_word}) = \"\n",
        "            f\"{bigram_counts[bigram]} / {unigram_counts[prev_word]} = {p_conditional}\"\n",
        "        )\n",
        "\n",
        "    print(\"\\n===== FINAL RESULT =====\")\n",
        "    print(f\"Final sentence probability: {probability}\")\n",
        "\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "UPmgDYaE5p-A"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I love comp sci \",\n",
        "    \"We love comp vis \",\n",
        "    \"They love comp sci \",\n",
        "    \"We hate comp sci \",\n",
        "]\n",
        "\n",
        "# sentence = \"we love comp sci \"\n",
        "sentence = \"WE LOVE MATHS  \"\n",
        "\n",
        "sentence_probability(corpus, sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mo13YZMO8sm6",
        "outputId": "86746334-cb6b-41c7-e49b-26d53f27fa63"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 4\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'I love comp sci ' -> Tokens: ['i', 'love', 'comp', 'sci']\n",
            "Sentence: 'We love comp vis ' -> Tokens: ['we', 'love', 'comp', 'vis']\n",
            "Sentence: 'They love comp sci ' -> Tokens: ['they', 'love', 'comp', 'sci']\n",
            "Sentence: 'We hate comp sci ' -> Tokens: ['we', 'hate', 'comp', 'sci']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'i': 1\n",
            "Unigram 'love': 3\n",
            "Unigram 'comp': 4\n",
            "Unigram 'sci': 3\n",
            "Unigram 'we': 2\n",
            "Unigram 'vis': 1\n",
            "Unigram 'they': 1\n",
            "Unigram 'hate': 1\n",
            "Total unigrams in corpus: 16\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('i', 'love'): 1\n",
            "Bigram ('love', 'comp'): 3\n",
            "Bigram ('comp', 'sci'): 3\n",
            "Bigram ('we', 'love'): 1\n",
            "Bigram ('comp', 'vis'): 1\n",
            "Bigram ('they', 'love'): 1\n",
            "Bigram ('we', 'hate'): 1\n",
            "Bigram ('hate', 'comp'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['we', 'love', 'maths']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY =====\n",
            "P(we) = 2 / 4 = 0.5\n",
            "P(love | we) = 1 / 2 = 0.5\n",
            "P(maths | love) = 0 (bigram not found)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Laplace"
      ],
      "metadata": {
        "id": "MctTeuFV-bfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_probability_lap(corpus, sentence):\n",
        "    list_len = len(corpus)\n",
        "    print(f\"Corpus length (number of sentences): {list_len}\")\n",
        "\n",
        "    print(\"===== STEP 1: TOKENIZE CORPUS =====\")\n",
        "    tokenized_corpus = []\n",
        "    for sent in corpus:\n",
        "        tokens = sent.lower().split()\n",
        "        tokenized_corpus.append(tokens)\n",
        "        print(f\"Sentence: '{sent}' -> Tokens: {tokens}\")\n",
        "\n",
        "    print(\"\\n===== STEP 2: COUNT UNIGRAMS =====\")\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sent in tokenized_corpus:\n",
        "        for word in sent:\n",
        "            unigram_counts[word] = unigram_counts.get(word, 0) + 1\n",
        "            total_unigrams += 1\n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        print(f\"Unigram '{word}': {count}\")\n",
        "\n",
        "    print(f\"Total unigrams in corpus: {total_unigrams}\")\n",
        "\n",
        "    # -------- LAPLACE CHANGE 1 --------\n",
        "    # Vocabulary size needed for Laplace smoothing\n",
        "    V = len(unigram_counts)\n",
        "    print(f\"Vocabulary size (V): {V}\")\n",
        "    # ---------------------------------\n",
        "\n",
        "    print(\"\\n===== STEP 3: COUNT BIGRAMS =====\")\n",
        "    bigram_counts = {}\n",
        "\n",
        "    for sent in tokenized_corpus:\n",
        "        for i in range(len(sent) - 1):\n",
        "            bigram = (sent[i], sent[i + 1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        print(f\"Bigram {bigram}: {count}\")\n",
        "\n",
        "    print(\"\\n===== STEP 4: TOKENIZE INPUT SENTENCE =====\")\n",
        "    sentence_tokens = sentence.lower().split()\n",
        "    print(f\"Input sentence tokens: {sentence_tokens}\")\n",
        "\n",
        "    print(\"\\n===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\")\n",
        "\n",
        "    probability = 1.0\n",
        "\n",
        "    # -------- LAPLACE CHANGE 2 --------\n",
        "    # P(w1) with Laplace smoothing\n",
        "    first_word = sentence_tokens[0]\n",
        "    first_word_count = unigram_counts.get(first_word, 0)\n",
        "\n",
        "    p_first = (first_word_count + 1) / ( list_len)\n",
        "    probability *= p_first\n",
        "\n",
        "    print(\n",
        "        f\"P({first_word}) = ({first_word_count} + 1) / \"\n",
        "        f\"({ list_len} ) = {p_first}\"\n",
        "    )\n",
        "    # ---------------------------------\n",
        "\n",
        "    # -------- LAPLACE CHANGE 3 --------\n",
        "    # Conditional probabilities with Laplace smoothing\n",
        "    for i in range(len(sentence_tokens) - 1):\n",
        "        prev_word = sentence_tokens[i]\n",
        "        curr_word = sentence_tokens[i + 1]\n",
        "        bigram = (prev_word, curr_word)\n",
        "\n",
        "        bigram_count = bigram_counts.get(bigram, 0)\n",
        "        prev_word_count = unigram_counts.get(prev_word, 0)\n",
        "\n",
        "        p_conditional = (bigram_count + 1) / (prev_word_count + V)\n",
        "        probability *= p_conditional\n",
        "\n",
        "        print(\n",
        "            f\"P({curr_word} | {prev_word}) = \"\n",
        "            f\"({bigram_count} + 1) / ({prev_word_count} + {V}) = {p_conditional}\"\n",
        "        )\n",
        "    # ---------------------------------\n",
        "\n",
        "    print(\"\\n===== FINAL RESULT =====\")\n",
        "    print(f\"Final sentence probability (with Laplace smoothing): {probability}\")\n",
        "\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "_xzoZF988-7H"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"I love comp sci \",\n",
        "    \"We love comp vis \",\n",
        "    \"They love comp sci \",\n",
        "    \"We hate comp sci \",\n",
        "]\n",
        "\n",
        "# sentence = \"we love comp sci \"\n",
        "sentence = \"WE LOVE MATHS  \"\n",
        "\n",
        "sentence_probability_lap(corpus, sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXQJVr-lBwf2",
        "outputId": "bdebc396-8cdf-44d5-bd2d-7f5a4538145f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length (number of sentences): 4\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'I love comp sci ' -> Tokens: ['i', 'love', 'comp', 'sci']\n",
            "Sentence: 'We love comp vis ' -> Tokens: ['we', 'love', 'comp', 'vis']\n",
            "Sentence: 'They love comp sci ' -> Tokens: ['they', 'love', 'comp', 'sci']\n",
            "Sentence: 'We hate comp sci ' -> Tokens: ['we', 'hate', 'comp', 'sci']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'i': 1\n",
            "Unigram 'love': 3\n",
            "Unigram 'comp': 4\n",
            "Unigram 'sci': 3\n",
            "Unigram 'we': 2\n",
            "Unigram 'vis': 1\n",
            "Unigram 'they': 1\n",
            "Unigram 'hate': 1\n",
            "Total unigrams in corpus: 16\n",
            "Vocabulary size (V): 8\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('i', 'love'): 1\n",
            "Bigram ('love', 'comp'): 3\n",
            "Bigram ('comp', 'sci'): 3\n",
            "Bigram ('we', 'love'): 1\n",
            "Bigram ('comp', 'vis'): 1\n",
            "Bigram ('they', 'love'): 1\n",
            "Bigram ('we', 'hate'): 1\n",
            "Bigram ('hate', 'comp'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['we', 'love', 'maths']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\n",
            "P(we) = (2 + 1) / (4 + 8) = 0.25\n",
            "P(love | we) = (1 + 1) / (2 + 8) = 0.2\n",
            "P(maths | love) = (0 + 1) / (3 + 8) = 0.09090909090909091\n",
            "\n",
            "===== FINAL RESULT =====\n",
            "Final sentence probability (with Laplace smoothing): 0.004545454545454546\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.004545454545454546"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i love comp sci\"\n",
        "\n",
        "sentence_probability_lap(corpus, sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b36bUboC27p",
        "outputId": "f197d3e4-3fde-430e-ebe4-c44442553723"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length (number of sentences): 4\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'I love comp sci ' -> Tokens: ['i', 'love', 'comp', 'sci']\n",
            "Sentence: 'We love comp vis ' -> Tokens: ['we', 'love', 'comp', 'vis']\n",
            "Sentence: 'They love comp sci ' -> Tokens: ['they', 'love', 'comp', 'sci']\n",
            "Sentence: 'We hate comp sci ' -> Tokens: ['we', 'hate', 'comp', 'sci']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'i': 1\n",
            "Unigram 'love': 3\n",
            "Unigram 'comp': 4\n",
            "Unigram 'sci': 3\n",
            "Unigram 'we': 2\n",
            "Unigram 'vis': 1\n",
            "Unigram 'they': 1\n",
            "Unigram 'hate': 1\n",
            "Total unigrams in corpus: 16\n",
            "Vocabulary size (V): 8\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('i', 'love'): 1\n",
            "Bigram ('love', 'comp'): 3\n",
            "Bigram ('comp', 'sci'): 3\n",
            "Bigram ('we', 'love'): 1\n",
            "Bigram ('comp', 'vis'): 1\n",
            "Bigram ('they', 'love'): 1\n",
            "Bigram ('we', 'hate'): 1\n",
            "Bigram ('hate', 'comp'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['i', 'love', 'comp', 'sci']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\n",
            "P(i) = (1 + 1) / (4 + 8) = 0.16666666666666666\n",
            "P(love | i) = (1 + 1) / (1 + 8) = 0.2222222222222222\n",
            "P(comp | love) = (3 + 1) / (3 + 8) = 0.36363636363636365\n",
            "P(sci | comp) = (3 + 1) / (4 + 8) = 0.3333333333333333\n",
            "\n",
            "===== FINAL RESULT =====\n",
            "Final sentence probability (with Laplace smoothing): 0.0044893378226711555\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0044893378226711555"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "'patient has fever',\n",
        "'patient has cough',\n",
        "'patient has headache',\n",
        "'doctor treats patient',\n",
        "'doctor gives medicine',\n",
        "'medicine reduces fever' ]"
      ],
      "metadata": {
        "id": "5hXB4nzLRcuJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no laplace\n",
        "sentence_probability(corpus, \"patient has fever\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORMweDcRRg6O",
        "outputId": "e19faafc-1306-4d79-a5e7-6555162ffe5d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length: 6\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'patient has fever' -> Tokens: ['patient', 'has', 'fever']\n",
            "Sentence: 'patient has cough' -> Tokens: ['patient', 'has', 'cough']\n",
            "Sentence: 'patient has headache' -> Tokens: ['patient', 'has', 'headache']\n",
            "Sentence: 'doctor treats patient' -> Tokens: ['doctor', 'treats', 'patient']\n",
            "Sentence: 'doctor gives medicine' -> Tokens: ['doctor', 'gives', 'medicine']\n",
            "Sentence: 'medicine reduces fever' -> Tokens: ['medicine', 'reduces', 'fever']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'patient': 4\n",
            "Unigram 'has': 3\n",
            "Unigram 'fever': 2\n",
            "Unigram 'cough': 1\n",
            "Unigram 'headache': 1\n",
            "Unigram 'doctor': 2\n",
            "Unigram 'treats': 1\n",
            "Unigram 'gives': 1\n",
            "Unigram 'medicine': 2\n",
            "Unigram 'reduces': 1\n",
            "Total unigrams in corpus: 18\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('patient', 'has'): 3\n",
            "Bigram ('has', 'fever'): 1\n",
            "Bigram ('has', 'cough'): 1\n",
            "Bigram ('has', 'headache'): 1\n",
            "Bigram ('doctor', 'treats'): 1\n",
            "Bigram ('treats', 'patient'): 1\n",
            "Bigram ('doctor', 'gives'): 1\n",
            "Bigram ('gives', 'medicine'): 1\n",
            "Bigram ('medicine', 'reduces'): 1\n",
            "Bigram ('reduces', 'fever'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['patient', 'has', 'fever']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY =====\n",
            "P(patient) = 4 / 6 = 0.6666666666666666\n",
            "P(has | patient) = 3 / 4 = 0.75\n",
            "P(fever | has) = 1 / 3 = 0.3333333333333333\n",
            "\n",
            "===== FINAL RESULT =====\n",
            "Final sentence probability: 0.16666666666666666\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0#laplace\n",
        "sentence_probability_lap(corpus, \"patient has fever\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1t88iWyRogs",
        "outputId": "d6df9f48-cc77-40d2-8b80-98adf39f072a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length (number of sentences): 6\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'patient has fever' -> Tokens: ['patient', 'has', 'fever']\n",
            "Sentence: 'patient has cough' -> Tokens: ['patient', 'has', 'cough']\n",
            "Sentence: 'patient has headache' -> Tokens: ['patient', 'has', 'headache']\n",
            "Sentence: 'doctor treats patient' -> Tokens: ['doctor', 'treats', 'patient']\n",
            "Sentence: 'doctor gives medicine' -> Tokens: ['doctor', 'gives', 'medicine']\n",
            "Sentence: 'medicine reduces fever' -> Tokens: ['medicine', 'reduces', 'fever']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'patient': 4\n",
            "Unigram 'has': 3\n",
            "Unigram 'fever': 2\n",
            "Unigram 'cough': 1\n",
            "Unigram 'headache': 1\n",
            "Unigram 'doctor': 2\n",
            "Unigram 'treats': 1\n",
            "Unigram 'gives': 1\n",
            "Unigram 'medicine': 2\n",
            "Unigram 'reduces': 1\n",
            "Total unigrams in corpus: 18\n",
            "Vocabulary size (V): 10\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('patient', 'has'): 3\n",
            "Bigram ('has', 'fever'): 1\n",
            "Bigram ('has', 'cough'): 1\n",
            "Bigram ('has', 'headache'): 1\n",
            "Bigram ('doctor', 'treats'): 1\n",
            "Bigram ('treats', 'patient'): 1\n",
            "Bigram ('doctor', 'gives'): 1\n",
            "Bigram ('gives', 'medicine'): 1\n",
            "Bigram ('medicine', 'reduces'): 1\n",
            "Bigram ('reduces', 'fever'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['patient', 'has', 'fever']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\n",
            "P(patient) = (4 + 1) / (6 + 10) = 0.3125\n",
            "P(has | patient) = (3 + 1) / (4 + 10) = 0.2857142857142857\n",
            "P(fever | has) = (1 + 1) / (3 + 10) = 0.15384615384615385\n",
            "\n",
            "===== FINAL RESULT =====\n",
            "Final sentence probability (with Laplace smoothing): 0.013736263736263736\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.013736263736263736"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_probability_lap(corpus, \"fever has patient\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR9Q_JmdU4ru",
        "outputId": "6de80865-38c4-46bb-9272-a6e09be02b73"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length (number of sentences): 6\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'patient has fever' -> Tokens: ['patient', 'has', 'fever']\n",
            "Sentence: 'patient has cough' -> Tokens: ['patient', 'has', 'cough']\n",
            "Sentence: 'patient has headache' -> Tokens: ['patient', 'has', 'headache']\n",
            "Sentence: 'doctor treats patient' -> Tokens: ['doctor', 'treats', 'patient']\n",
            "Sentence: 'doctor gives medicine' -> Tokens: ['doctor', 'gives', 'medicine']\n",
            "Sentence: 'medicine reduces fever' -> Tokens: ['medicine', 'reduces', 'fever']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'patient': 4\n",
            "Unigram 'has': 3\n",
            "Unigram 'fever': 2\n",
            "Unigram 'cough': 1\n",
            "Unigram 'headache': 1\n",
            "Unigram 'doctor': 2\n",
            "Unigram 'treats': 1\n",
            "Unigram 'gives': 1\n",
            "Unigram 'medicine': 2\n",
            "Unigram 'reduces': 1\n",
            "Total unigrams in corpus: 18\n",
            "Vocabulary size (V): 10\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('patient', 'has'): 3\n",
            "Bigram ('has', 'fever'): 1\n",
            "Bigram ('has', 'cough'): 1\n",
            "Bigram ('has', 'headache'): 1\n",
            "Bigram ('doctor', 'treats'): 1\n",
            "Bigram ('treats', 'patient'): 1\n",
            "Bigram ('doctor', 'gives'): 1\n",
            "Bigram ('gives', 'medicine'): 1\n",
            "Bigram ('medicine', 'reduces'): 1\n",
            "Bigram ('reduces', 'fever'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['fever', 'has', 'patient']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\n",
            "P(fever) = (2 + 1) / (6 + 10) = 0.1875\n",
            "P(has | fever) = (0 + 1) / (2 + 10) = 0.08333333333333333\n",
            "P(patient | has) = (0 + 1) / (3 + 10) = 0.07692307692307693\n",
            "\n",
            "===== FINAL RESULT =====\n",
            "Final sentence probability (with Laplace smoothing): 0.001201923076923077\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.001201923076923077"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "\"patient has fever\",\n",
        "'fever has patient',\n",
        "'doctor is medicine',\n",
        "'fever is patient'\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "sn_NqZ35QWSd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K smoothning"
      ],
      "metadata": {
        "id": "qLs80iZjTfUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_probability_k(corpus, sentence , k ):\n",
        "    list_len = len(corpus)\n",
        "    print(f\"Corpus length (number of sentences): {list_len}\")\n",
        "\n",
        "    print(\"===== STEP 1: TOKENIZE CORPUS =====\")\n",
        "    tokenized_corpus = []\n",
        "    for sent in corpus:\n",
        "        tokens = sent.lower().split()\n",
        "        tokenized_corpus.append(tokens)\n",
        "        print(f\"Sentence: '{sent}' -> Tokens: {tokens}\")\n",
        "\n",
        "    print(\"\\n===== STEP 2: COUNT UNIGRAMS =====\")\n",
        "    unigram_counts = {}\n",
        "    total_unigrams = 0\n",
        "\n",
        "    for sent in tokenized_corpus:\n",
        "        for word in sent:\n",
        "            unigram_counts[word] = unigram_counts.get(word, 0) + 1\n",
        "            total_unigrams += 1\n",
        "\n",
        "    for word, count in unigram_counts.items():\n",
        "        print(f\"Unigram '{word}': {count}\")\n",
        "\n",
        "    print(f\"Total unigrams in corpus: {total_unigrams}\")\n",
        "\n",
        "    # -------- LAPLACE CHANGE 1 --------\n",
        "    # Vocabulary size needed for Laplace smoothing\n",
        "    V = len(unigram_counts)\n",
        "    print(f\"Vocabulary size (V): {V}\")\n",
        "    # ---------------------------------\n",
        "\n",
        "    print(\"\\n===== STEP 3: COUNT BIGRAMS =====\")\n",
        "    bigram_counts = {}\n",
        "\n",
        "    for sent in tokenized_corpus:\n",
        "        for i in range(len(sent) - 1):\n",
        "            bigram = (sent[i], sent[i + 1])\n",
        "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
        "\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        print(f\"Bigram {bigram}: {count}\")\n",
        "\n",
        "    print(\"\\n===== STEP 4: TOKENIZE INPUT SENTENCE =====\")\n",
        "    sentence_tokens = sentence.lower().split()\n",
        "    print(f\"Input sentence tokens: {sentence_tokens}\")\n",
        "\n",
        "    print(\"\\n===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\")\n",
        "\n",
        "    probability = 1.0\n",
        "\n",
        "    # -------- LAPLACE CHANGE 2 --------\n",
        "    # P(w1) with Laplace smoothing\n",
        "    first_word = sentence_tokens[0]\n",
        "    first_word_count = unigram_counts.get(first_word, 0)\n",
        "\n",
        "    p_first = (first_word_count + k) / ( list_len + (k*V))\n",
        "    probability *= p_first\n",
        "\n",
        "    print(\n",
        "        f\"P({first_word}) = ({first_word_count} + 1) / \"\n",
        "        f\"({ list_len} ) = {p_first}\"\n",
        "    )\n",
        "    # ---------------------------------\n",
        "\n",
        "    # -------- LAPLACE CHANGE 3 --------\n",
        "    # Conditional probabilities with Laplace smoothing\n",
        "    for i in range(len(sentence_tokens) - 1):\n",
        "        prev_word = sentence_tokens[i]\n",
        "        curr_word = sentence_tokens[i + 1]\n",
        "        bigram = (prev_word, curr_word)\n",
        "\n",
        "        bigram_count = bigram_counts.get(bigram, 0)\n",
        "        prev_word_count = unigram_counts.get(prev_word, 0)\n",
        "\n",
        "        p_conditional = (bigram_count + k) / (prev_word_count + (k*V))\n",
        "        probability *= p_conditional\n",
        "\n",
        "        print(\n",
        "            f\"P({curr_word} | {prev_word}) = \"\n",
        "            f\"({bigram_count} + 1) / ({prev_word_count} + {k*V}) = {p_conditional}\"\n",
        "        )\n",
        "    # ---------------------------------\n",
        "\n",
        "    print(\"\\n===== FINAL RESULT =====\")\n",
        "    print(f\"Final sentence probability (with Laplace smoothing): {probability}\")\n",
        "\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "IEYzgX_NSPdl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_probability_k(corpus, \"patient has fever\" , 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsIwBs9zUGUV",
        "outputId": "7e63cb83-b8f2-4d6b-d234-2aff3dc41679"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length (number of sentences): 6\n",
            "===== STEP 1: TOKENIZE CORPUS =====\n",
            "Sentence: 'patient has fever' -> Tokens: ['patient', 'has', 'fever']\n",
            "Sentence: 'patient has cough' -> Tokens: ['patient', 'has', 'cough']\n",
            "Sentence: 'patient has headache' -> Tokens: ['patient', 'has', 'headache']\n",
            "Sentence: 'doctor treats patient' -> Tokens: ['doctor', 'treats', 'patient']\n",
            "Sentence: 'doctor gives medicine' -> Tokens: ['doctor', 'gives', 'medicine']\n",
            "Sentence: 'medicine reduces fever' -> Tokens: ['medicine', 'reduces', 'fever']\n",
            "\n",
            "===== STEP 2: COUNT UNIGRAMS =====\n",
            "Unigram 'patient': 4\n",
            "Unigram 'has': 3\n",
            "Unigram 'fever': 2\n",
            "Unigram 'cough': 1\n",
            "Unigram 'headache': 1\n",
            "Unigram 'doctor': 2\n",
            "Unigram 'treats': 1\n",
            "Unigram 'gives': 1\n",
            "Unigram 'medicine': 2\n",
            "Unigram 'reduces': 1\n",
            "Total unigrams in corpus: 18\n",
            "Vocabulary size (V): 10\n",
            "\n",
            "===== STEP 3: COUNT BIGRAMS =====\n",
            "Bigram ('patient', 'has'): 3\n",
            "Bigram ('has', 'fever'): 1\n",
            "Bigram ('has', 'cough'): 1\n",
            "Bigram ('has', 'headache'): 1\n",
            "Bigram ('doctor', 'treats'): 1\n",
            "Bigram ('treats', 'patient'): 1\n",
            "Bigram ('doctor', 'gives'): 1\n",
            "Bigram ('gives', 'medicine'): 1\n",
            "Bigram ('medicine', 'reduces'): 1\n",
            "Bigram ('reduces', 'fever'): 1\n",
            "\n",
            "===== STEP 4: TOKENIZE INPUT SENTENCE =====\n",
            "Input sentence tokens: ['patient', 'has', 'fever']\n",
            "\n",
            "===== STEP 5: CALCULATE PROBABILITY (WITH LAPLACE) =====\n",
            "P(patient) = (4 + 1) / (6 ) = 0.5857142857142856\n",
            "P(has | patient) = (3 + 1) / (4 + 1.0) = 0.62\n",
            "P(fever | has) = (1 + 1) / (3 + 1.0) = 0.275\n",
            "\n",
            "===== FINAL RESULT =====\n",
            "Final sentence probability (with Laplace smoothing): 0.09986428571428571\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09986428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HxgKA-YWUNTc"
      },
      "execution_count": 47,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}